\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{subcaption}

\usepackage{tikz}
\usepackage{pgfplots}

\usepackage{cleveref}
\usepackage{csquotes}
\usepackage{booktabs}

\usepackage{flushend}

\usetikzlibrary{patterns}

\definecolor{tol1}{HTML}{33bbee}
\definecolor{tol2}{HTML}{009988}
\definecolor{tol3}{HTML}{ee7733}
\definecolor{tol4}{HTML}{cc3311}
\definecolor{tol5}{HTML}{66CCEE}
\definecolor{tol6}{HTML}{CD7F32}

\definecolor{plt1}{HTML}{4053d3}
\definecolor{plt2}{HTML}{ddb310}
\definecolor{plt3}{HTML}{b51d14}
\definecolor{plt4}{HTML}{00beff}
\definecolor{plt5}{HTML}{fb49b0}

\pgfplotscreateplotcyclelist{bwb}{%
  thick, tol1\\%
  thick, tol2\\%
  thick, tol3\\%
  thick, tol4\\%
  thick, tol5\\%
  thick, tol6\\%
}

\usepackage{acro}
\DeclareAcronym{cots}{
  short = COTS,
  long = off-the-shelf,
}

\newcommand\MP{\ensuremath{\mathrm{MP}}\xspace}
\newcommand\SNR{\ensuremath{\mathrm{SNR}}\xspace}

\newcommand\ours{\textsc{AdJoCo}\xspace}
\newcommand\baseline{Baseline\xspace}

\DeclareAcronym{tiff}{
  short = TIFF,
  long = tag image file format,
}

\DeclareAcronym{mse}{
  short = MSE,
  long = mean squared error,
}

\DeclareAcronym{awgn}{
  short = AWGN,
  long = additive white Gaussian noise,
}

\DeclareAcronym{pdf}{
  short = PDF,
  long = probability-density function
}

\DeclareAcronym{leo}{
  short = LEO,
  long = low Earth orbit,
}

\DeclareAcronym{ldpc}{
  short = LDPC,
  long = low-density parity-check,
}

\DeclareAcronym{jscc}{
  short = JSCC,
  long = joint source and channel coding,
}

\DeclareAcronym{djscc}{
  short = DJSCC,
  long = deep joint source and channel coding,
}

\DeclareAcronym{snr}{
  short = SNR,
  long = signal-to-noise ratio,
}

\DeclareAcronym{cnn}{
  short = CNN,
  long = convolutional neural network,
}

\DeclareAcronym{psnr}{
  short = PSNR,
  long = peak signal-to-noise ratio,
}

\DeclareAcronym{prelu}{
  short = PReLU,
  long = parameterized rectified linear unit,
}

\DeclareAcronym{relu}{
  short = ReLU,
  long = rectified linear unit,
}

\DeclareAcronym{jscc-sat}{
  short = \textsc{JSCC-Sat},
  long = {joint source-and-channel coding for small satellite applications}
}

\DeclareAcronym{los}{
  short = {LOS},
  long = {line of sight}
}

\DeclareAcronym{esa}{
  short=ESA,
  long=European Space Agency
}

\usepackage{xspace}
\newcommand\cubesat{CubeSat\xspace}
\newcommand\cubesats{\cubesat{}s\xspace}

\newcommand\jpegtwok{JPEG\,2000\xspace}

\newcommand\sentinelii{Sentinel-2\xspace}

\begin{document}

\title{Adaptable Joint Source-and-Channel Coding for Small Satellite Applications}

\author{\emph{Anonymous authors}}
% \author{\IEEEauthorblockN{Olga Kondrateva}
% \IEEEauthorblockA{\textit{Humboldt-Universit\"at zu Berlin}\\
% Berlin, Germany \\
% kondrate@informatik.hu-berlin.de}
% \and
% \IEEEauthorblockN{Stefan Dietzel}
% \IEEEauthorblockA{\textit{Merantix Momentum GmbH}\\
% Berlin, Germany \\
% stefan@merantix-momentum.com}
% \and
% \IEEEauthorblockN{Bj\"orn Scheuermann}
% \IEEEauthorblockA{\textit{Technical University of Darmstadt}\\
% Darmstadt, Germany \\
% scheuermann@kom.tu-darmstadt.de}
% }

\maketitle

\begin{abstract}
Earth observation using small satellites serves a wide range of relevant applications.
However, significant advances in sensor technology (e.g., higher resolution, multiple spectrums beyond visible light) in combination with challenging channel characteristics lead to a communication bottleneck 
when transmitting the collected data to Earth.
Recently, joint source coding, channel coding, and modulation using a neuronal-network-based approach has been proposed to combine image compression and communication. %dynamically adapt to the varying communication channel and to take data properties into account.
Though this approach achieves promising results when applied to standard terrestrial channel models, 
it remains an open question whether it is suitable for the more complicated and quickly varying satellite communication channel. 
In this paper, we consider a detailed satellite channel model accounting for different shadowing conditions and train an encoder-decoder architecture using realistic \sentinelii satellite imagery.
In addition, to reduce the overhead associated with using multiple neural networks for various channel states, 
we leverage attention modules and train a single adaptable neural network that covers a wide range of different channel conditions.
Our evaluation results show that the proposed approach achieves on-par performance when compared to less space efficient schemes using separate neuronal networks for differing channel conditions.   
% it typically involves training multiple neural networks to account for different channel conditions  
% In this paper, we propose a novel joint coding scheme that combines image compression, channel coding, and modulation using a neuronal-network-based approach.
% By combining a detailed channel model for small satellite applications with attention modules, we can use a single adaptable neuronal network to cover a wide range of different channel conditions.
% We evaluate our approach using realistic \sentinelii satellite imagery and show that it achieves on par performance when compared to less space efficient schemes using separate neuronal networks for differing channel conditions.
\end{abstract}

\begin{IEEEkeywords}
  cross-layer optimization; AI-enabled networking; small satellite applications
  \end{IEEEkeywords}

\acresetall
\section{Introduction}

Earth observation using sensor data acquired by satellites has gained more and more attention over the last years.
Common use cases include environment monitoring \cite{rs14030589}, disaster management \cite{barmpoutis2020}, and many more \cite{radix,MarCO}.
The increasing momentum can be explained by two major factors.
First, technological advances have allowed to build smaller satellite classes, which use more off-the-shelf components and can be deployed more easily.
A prime example are CubeSats, which operate in \ac{leo} and consist of $10 \times 10 \times 10$\,cm units \cite{cubesat2020}.
Second, sensor technology has improved greatly.
Besides higher resolution, modern sensors support a wider spectrum range, exceeding that of visible light.
Hyper-spectral images include infrared and other bands, which can be used monitor vegetation, clouds, and other phenomena not represented in the visible light spectrum.

These advances in satellite and sensor technology, however, are not met by an equal improvement in communication capacity.
CubeSats and other small satellites have a constrained energy budget, and -- unlike their larger counterparts -- they are not geo-stationary.
That is, they orbit the Earth several times per day with high speed, limiting communication with ground stations to several short communication windows.
Their high velocity and interferences due to harsh weather and potential non line-of-sight conditions further lead to high packet loss rates and complicate communication \cite{nogales2018}.

Therefore, efficient and robust coding schemes are a necessity to support demanding Earth observation applications.
Typically, source coding, channel coding, and modulation schemes are combined to translate image sensor data into physical layer channel symbols.
Source coding serves to compress sensor images, often using lossy compression schemes, such as \jpegtwok \cite{sentinel-2-user-handbook}.
Channel coding (e.g., \ac{ldpc}) is then used to enable error correction, counteracting packet loss due to the harsh channel conditions.
More recently, approaches that consider these coding mechanisms jointly have emerged, promising better than performance than using individual coding schemes \cite{6408177}.
Joint coding leverages the data-centric nature of the considered use case: the characteristics of the hyper-spectral image sensor data is well known, and therefore, the image data can be translated directly into physical layer symbols rather than using separate source codes, channel codes, and modulation.
And although Shannon's theory \cite{cover1991elements} states that separate optimization should yield optimal results, this is not true in practice due to hypothetical assumptions, such as infinite code block lengths.

The use of neuronal networks has provided a feasible way to implement such a joint coding approach, improving over earlier work, which was too complex to be useful in practice \cite{9838671}.
Neuronal-network-based joint coding approaches have been proposed for both terrestrial communication \cite{Bourtsoulatze2019} and satellite applications \cite{satjscc}.
So far, a major limitation has been that the joint encoder and decoder has been trained based on fixed assumptions about channel characteristics, such as the \ac{snr} using a simple \ac{awgn} channel model.
To accommodate changing channel conditions, separate neuronal networks need to be trained independently and switched between by both the satellite and the ground station.
Obviously, this approach restricts the total number of channel parameter combinations that can reasonably be covered.
Considering more complex channel models including multi-path propagation, shadowing, and fading would easily lead to a combinatorial explosion of parameters, and consequently a prohibitive amount of separate neuronal networks.

In this paper, we propose a novel joint coding approach for satellite applications that allows to use a single neuronal network for a wide range of channel conditions typical for small satellite applications.
We use a neuronal network model architecture that is enriched by so-called attention modules to reduce the combinatorial complexity.
Using this architecture, we can train a single neuronal network based on a number of different channel conditions.
During operation, the attention modules allow to essentially parametrize the network for different actual channel conditions.
This parametrization allows us to consider a wide range of realistic channel conditions for small satellite applications.
To model the channel, we use Fontán et al.'s channel model \cite{fontan2001}, which is applicable to non-geostationary small satellites and models a number of channel characteristics, such as multi-path propagation and shadowing.
More specifically, we consider three scenarios -- \ac{los}, shadowing, and deep shadowing -- to capture different extents of multi-path propagation and shadowing effects.
In addition, we also consider different levels of \ac{snr} to model satellite elevation angles and other parameters.
During training of the encoder-decoder neuronal network, these parameters are used as input, in addition to a large number of example satellite images.
Whereas a traditional encoder-decoder network would then -- simply speaking -- learn a coding scheme for the \enquote{average} of all parameters, the attention modules allow to embed schemes for all parameters within a single network that can be dynamically parameterized with the actual channel conditions during use.
Therefore, a single network suffices when using attention modules, and separate networks with a simpler architecture are required in order to precisely represent different channel conditions.
Although the resulting network with attention modules is larger than one trained for a specific set of channel conditions, it is considerably smaller than considering a set of separate networks, one for each channel condition.

Our evaluation using a set of hyper-spectral images from the \sentinelii mission shows that our approach performs as good as separate, individual networks for different channel conditions while requiring significantly lower storage overhead.

Thus, our main contributions can be summarized as follows:
%
\begin{enumerate}
  \item We combine \ac{jscc} approaches with a realistic channel model for small satellite applications.
  \item We apply an attention-module-augmented neuronal network architecture to be able to use a single network for a wide range of realistic channel characteristics.
  \item We evaluate our approach using a set of realistic \sentinelii Earth observation data.
\end{enumerate}

The remainder of this paper is organized as follows.
In \Cref{sec:related_work}, we review existing work on \ac{jscc}, source coding, channel models, and attention modules.
Next, we provide an overview of our system model in \Cref{sec:system_model} before explaining our mechanism in detail in \Cref{sec:our_approach}.
\Cref{sec:evaluation} details our evaluation results using \sentinelii mission data.
We conclude the paper in \Cref{sec:conclusion}.

\section{Related Work}
\label{sec:related_work}

% \begin{itemize}
%   \item LCN23
%   \item Channel models for satellites
%   \item Attention modules
% \end{itemize}

%Shannon
%JSCC without deep learing
% DJSCC separate source, separate channel, joint
% Aber hat noch keiner mit sat ausprobiert -> canal modelle
% Attention modules
% Channel mismatch

Source and channel coding are two essential components of a communication system with obviously interdependent objectives. While the goal of source coding is to reduce the amount of data to be transmitted by eliminating redundancies, channel coding adds redundancies to protect the data against possible transmission errors.  
Combining source and channel coding has been an active research topic over the last decades. Gallager gave the lower bound of lossless JSCC~\cite{gallager1968information}. Zhong et al. provide a systematic comparison between JSCC and SSCC error exponents~\cite{1614076} and derive lower and upper bounds for the error exponent for asymmetric two-user source-channel system~\cite{4557472}.
Apart from theoretical studies, various practical JSCC schemes have been proposed.
Yu et al. use optimization techniques to jointly optimize source and channel coding parameters to minimize energy consumption~\cite{Wei2004}. 
Xu et al. consider distributed JSCC for video transmission~\cite{4205066}. 

Recently, deep-learning-based methods have been successfully applied for source coding as well as for channel coding. In particular, various studies show that deep source coding is able to outperform traditional source coding techniques, such as JPEG and JPEG2000~\cite{toderici2016variable, ballé2017endtoend, Hu2022}.
Similarly, in the field of channel coding, O'Shea et al. have demonstrated that performance close to Hamming code can be achieved using an encoder-decoder architecture~\cite{8054694}. 
Based on these results, the use of deep learning has been extended to JSCC.
One of the first encoder-decoder architectures combining source coding, channel coding and modulation have been proposed by Bourtsoulatze et al.~\cite{Bourtsoulatze2019}. Later works addressed the limitations of this approach by considering such topics as correlated sources~\cite{Xuan2021}, progressive image transmission~\cite{Kurka2021} and adaptive rate control~\cite{yang2021_2}.
Also, the use of attention modules has been proposed to dynamically adapt to different SNR levels~\cite{9438648}. 
Furthermore, Kondrateva et al. proposed a JSCC encoder-decoder architecture suitable for transmission of satellite images. In contrast to our approach, these works limit their consideration to simplistic channel models that do not take into account the specifics of satellite communication.



\section{Earth Observation Missions}
\label{sec:system_model}

\begin{figure*}
  \begin{subfigure}{.32\linewidth}
    \includegraphics[width=\linewidth]{figures/Earth_from_Space_Southeast_Kenya}
    \caption{Southeast Kenya}
    \label{fig:sentinel_kenya}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.32\linewidth}
    \includegraphics[width=\linewidth]{figures/Saharan_dust_plume}
    \caption{Sahara dust}
    \label{fig:sentinel_sahara}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.32\linewidth}
    \includegraphics[width=\linewidth]{figures/Wildfires_continue_to_rage_in_Greece}
    \caption{Wildfires in Greece}
    \label{fig:sentinel_greece}
  \end{subfigure}

  \caption{Example images from the \sentinelii mission. (Credit: processed by ESA, CC BY-SA 3.0 IGO)}
  \label{fig:sentinelii}
\end{figure*}

\begin{table}
  \caption{\sentinelii Spectrum Bands}
  \label{tab:sentinel_bands}

  \centering
  \begin{tabular}{llcc}
    \toprule
    Band & Measurement & Central wavelength & Resolution \\
    \midrule
    1 & Coastal aerosol & 443 nm & 60 m \\
    2 & Blue & 490 nm & 10 m \\
    3 & Green & 560 nm & 10 m \\
    4 & Red & 665 nm & 10 m \\
    5 & Vegetation red edge & 705 nm & 20 m \\
    6 & Vegetation red edge & 740 nm & 20 m \\
    7 & Vegetation red edge & 783 nm & 20 m \\
    8 & Near infrared & 842 nm & 10 m \\
    8A & Narrow near infrared & 865 nm & 20 m \\
    9 & Water vapour & 940 nm & 60 m \\
    10 & Short wave infrared: cirrus & 1375 nm & 60 m \\
    11 & Short wave infrared & 1610 nm & 20 m \\
    12 & Short wave infrared & 2190 nm & 20 m \\
    \bottomrule
  \end{tabular}
  
\end{table}

With recent advances in space and image sensing technologies, Earth observation using small satellites has gained more and more attention over the past years.
As an example use case for the methods we propose in this paper, we briefly introduce ESA's \sentinelii mission.
We also use a subset of the mission's dataset for our evaluation.
Moreover, we give an overview of how neuronal networks are used onboard satellites for image processing.

\sentinelii \cite{sentinel2} is a set of missions operated by the \ac{esa} as part of the Copernicus program using \ac{leo} satellites.
The first satellites were launched in 2015, with more launches following in 2017 and 2024.
The missions comprise two satellites, which orbit the Earth such that each spot on the surface is revisited approximately every five days.
With their sensors, they cover a strip of land that is 290 km wide with each pass.
In addition to visible light sensors, the satellites are equipped with sensors that cover additional frequencies, such as infrared \cite{sentinel-2-user-handbook}, which allow to capture land use and vegetation (cf. \Cref{tab:sentinel_bands}).
Moreover, the sensors feature relatively high optical resolutions between 10 and 60 meters.

% https://www.esa.int/ESA_Multimedia/Images/2024/03/Earth_from_Space_Southeast_Kenya
% https://www.esa.int/ESA_Multimedia/Images/2024/04/Saharan_dust_plume
% https://www.esa.int/ESA_Multimedia/Images/2023/08/Wildfires_continue_to_rage_in_Greece

\Cref{fig:sentinelii} shows three example use cases for Earth observation images taken from \ac{esa}'s homepage.
The first (\Cref{fig:sentinel_kenya}) is a false-color image of southeast Kenya, which was generated by overlaying \sentinelii's near-infrared channel onto the visual spectrum.
The bright red colors indicate higher plant density and health, as alive plants reflect near-infrared light.
Thereby, the dense vegetation in the coastal regions can easily be distinguished from the hinterland regions.
\Cref{fig:sentinel_sahara} shows a dust storm originating from the Sahara desert.
\sentinelii provides valuable insights for air pollution monitoring, and due to the short revisit time, storms can be monitored as they develop.
Finally, \Cref{fig:sentinel_greece} shows wildfires in Greece in 2023.
For the visualization, the shortwave infrared spectrum was merged with the visible light spectrum, showing the fire front.
Dark brown areas show the burned area.
Thereby, these images provided valuable insights for civil protection authorities.
By using more efficient and more robust image compression and transmission, sensor data in small satellite missions can be transmitted and used faster, allowing even more rapid responses.

In this paper, we propose a neuronal-network-based \ac{jscc} approach to improve communication.
As small satellites are severely power-constrained, necessary hardware resources need to be taken into account when considering the feasibility of our solution.
Recently, a number of processing platforms have been successfully evaluated for use in space and small satellite applications.
Two examples are the Intel Movidius Myriad 2 and STM32 Microcontrollers, which have been used to identify stars and only use approximately 1 Watt power \cite{8556744}.
During the $\Phi$-Sat mission, deployment of machine learning models has been evaluated using an Intel Movidius Myriad processor, as well.
Similarly, Nvidia's TX2 SoC, which is compatible with the CubeSat standard's power constraints, has been used to detect cargo ships \cite{8556744}.
Finally, even relatively large standard machine learning models, such as VGG19 \cite{DBLP:journals/corr/SimonyanZ14a} and ResNet50 \cite{7780459}, have been evaluated on the International Space Station (ISS), operating on the Qualcomm Snapdragon 855 and Intel Movidius Myriad X processors \cite{9884906}.


\section{Our Approach}
\label{sec:our_approach}

The goal of our approach is to transmit sensor data $x \in \mathbb{R}$ acquired by small satellites in \ac{leo} to a ground station over a bandwidth-constrained channel.
The sensor data comprises optical images from multiple spectrums as defined in \Cref{sec:system_model}.
Assuming lossy compression and transmission, the ground station reconstructs an approximation $\hat{x}$ of the original data $x$ with the goal that $\hat{x}$ is as close to $x$ as possible.
Next, we given an overview of the protocol architecture before we explain the neural network architecture using attention modules and the channel model in more detail.

\subsection{Architecture overview}

\begin{figure}
  \includegraphics[width=\linewidth]{figures/coding-decoding}

  \caption{Overview of our communication architecture and comparison with traditional separate-encoder designs.}
  \label{fig:overview}
\end{figure}

To introduce our protocol's main components, we first explain the basic \ac{jscc} communication architecture and contrast it to the traditional case that uses separate encoders and decoders.
\Cref{fig:overview} shows an overview of the architecture in contrast to traditional approaches.
With separate coding (shown in the bottom half), the sensor data $x \in \mathbb{R}^n$ is first encoded using a source encoder, such as \jpegtwok.
Afterwards, a channel encoder (e.g., \ac{ldpc}) adds redundancy to the compressed signal in order to protect it against packet loss due to interference.
The modulator component then translates the channel encoder's output to physical layer samples $z \in \mathbb{C}^k$, which can be transmitted over a noisy channel.
The ground station receives the distorted signal $\hat{z}$ and uses corresponding components in inverse order to decode an approximation of $x$.
The demodulator $D_m$ translates the samples back to bits, which serve as input to the channel decoder $D_c$.
The channel decoder reconstructs the compressed image data, correcting transmission errors as far as possible.
The source decoder $D_s$ finally approximately reconstructs the original optical sensor data as $\hat{x}$.

In our \ac{jscc} approach (upper half of \Cref{fig:overview}), in contrast, parts of a single neuronal network are used as encoder $N_e$ and decoder $N_d$ by the satellite and ground station, respectively.
These components jointly perform the source coding, channel coding, modulation, and their corresponding inverse operations.

We use an encoder-decoder neuronal network architecture based on \cite{satjscc} to implement $N_e$ and $N_d$:
During training, the encoder ($N_e$) and decoder ($N_d$) parts of the network are linked together using a realistic channel model for noisy satellite links.
To better reflect realistic channel conditions, we replace the \ac{awgn} channel used in \cite{satjscc} with a more realistic channel model for \ac{leo} satellites.
The encoder-decoder model is then trained using an image dataset derived from the body of \sentinelii mission data.
The images within the dataset are used as model input and the reconstructed image's quality is used as reward metric.
Normally, a separate model would need to be trained for each characteristic set of channel conditions \cite{satjscc}.
In order to use a single network for a wide array of channel conditions, we augment the model with so-called attention modules \cite{wireless-attention-modules} as part of both the encoder and decoder during training.
These essentially allow to parameterize the network during later operation to cater to changing channel conditions.
The trained network is then separated into the encoder component $N_e$, which is used by the satellite and the decoder component $N_d$, which is used by the ground station.

\subsection{Encoder-decoder architecture with attention modules}

\begin{figure}
  \includegraphics[width=\linewidth]{figures/network-architecture}

  \caption{Network architecture overview.}
  \label{fig:architecture-overview}
\end{figure}

The encoder-decoder network architecture is shown in \Cref{fig:architecture-overview}.
As basis, we use the network structure proposed in \cite{satjscc}, which adapts ResNet \cite{resnet} to the \ac{jscc} use case.
To make the architecture more flexible, we add attention modules, as well as a more realistic channel model.
The block on the left serves as neural network encoder, jointly performing source coding, channel coding, and modulation.
Compression is achieved by translating the input $x \in \mathbb{R}^n$ to channel symbols $z \in \mathbb{C}^k$, where $k < n$.
By adjusting $k$, the system's compression ratio $k/n$ can be defined.

\begin{figure*}
  \begin{subfigure}{.3\linewidth}
    \centering
    \includegraphics[height=10em]{figures/residual-block}
    \caption{Residual block $(f, s)$}
    \label{fig:residual}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.3\linewidth}
    \centering
    \includegraphics[height=10em]{figures/residual-block-transpose}
    \caption{Residual block transpose $(f, s)$}
    \label{fig:residual-transpose}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.3\linewidth}
    \centering
    \includegraphics[height=10em]{figures/attention-module}
    \caption{Attention module}
    \label{fig:attention}
  \end{subfigure}

  \caption{Specific block architectures used within the encoder-decoder network.}
  \label{fig:blocks}
\end{figure*}

For the encoder, we use four residual blocks with 256 filters and a kernel size of $3 \times 3$.
The structure of the residual blocks is shown in \Cref{fig:residual}.
Each residual block is followed by an attention module as proposed in \cite{wireless-attention-modules}.
\Cref{fig:attention} shows the structure of the attention modules.
These modules allow to alter the feature weights of their preceding residual blocks in order to accommodate different channel conditions.
To do so, a number of channel parameters are added as additional input to the attention modules.
Based on these parameters, a number of scaling parameters are learned that either increase or decrease the connection strength to the next module, depending on the specific channel condition.
The attention modules operate in three steps.
First, global average pooling is applied to the previous residual block's output, which serves to make available global context information.
The pooled output is then concatenated with variables representing channel parameters.
Second, a simple neural network structure -- two fully connected layers plus \ac{relu} and sigmoid as activation functions -- is used to predict the proper scaling factors based on the channel condition parameters.
Finally, the previous residual layer's output is multiplied with the predicted scaling factors to implement the attention-based scaling.

As last layer of the encoder component, we use a convolutional layer that contains $c$ filters.
The value for $c$ is chosen based on the desired compression ratio $k/n$.
Finally, \ac{prelu} is used as activation function.
The result is a vector $\tilde{z}$ comprising $k$ complex numbers, which represent the channel symbols.

To learn and predict the actual channel conditions, the middle component of the encoder-decoder architecture ia a non-trainable channel layer, which models a realistic representation of the channel conditions in satellite communication.
For the channel, we assume a log-normal distribution with mean $\alpha$ and standard deviation $\psi$, a Rayleigh-distributed multi-path component \MP, and a signal-to-noise parameter \SNR.
We present the channel model in more detail in \Cref{sub:channel_model}.

The block on the right serves as neural network decoder, which translates the -- potentially corrupted channel symbols $\hat{z}$ back to an approximation $\hat{x}$ of the original data.
The decoder follows the same architecture design as the encoder: a convolutional transpose layer is followed by four residual transpose blocks plus corresponding attention modules,  another convolutional transpose layer, and a \ac{prelu} activation function.

As reward function during model training and to evaluate the quality of the reconstructed image signal, we use the \ac{psnr} metric:
%
\begin{equation}
  \mathrm{PSNR} = 10 \log_{10}\frac{\mathrm{MAX}^2}{\mathrm{MSE}},
\end{equation}
%
where $\mathrm{MAX}$ is the maximum possible pixel value.
The \ac{psnr} captures to what extent original signal value is affected by distorting noise as a fraction between the maximum signal and noise.


\subsection{Channel model for \ac{leo} satellites}
\label{sub:channel_model}

We compute the channel output symbols $\hat{z}$ as follows:
\begin{equation}
  \hat{z} = zh + n,
\end{equation}
where $h$ denotes channel gain and $n$ Gaussian noise.
Next, we explain how $h$ and $n$ are calculated.

To account for specific satellite channel conditions, we compute $h$ using Fontan's statistical channel~\cite{} model. 
Since the satellite channel characteristics heavily depend on shadowing conditions, in general, 
they cannot be accurately modeled using a single distribution.
Typically, multiple states describing different degrees of scheduling are introduced 
and the overall pdf is given as a sum of of the individual state pdfs multiplied by the probabilities of being in a given state.
For example, assuming a two-state model with \ac{los} and shadow conditions: 
\begin{equation}
f_{\mathrm{overall}}(r) = p_{\mathrm{los}}f_{\mathrm{los}}(r) + p_{\mathrm{shadow}}f_{\mathrm{shadow}}(r)
\end{equation}

Furthermore, to add more details, e.g. state duration, Markov chain models can be introduced.
Then, the propagation channel at a given time is described by the state probability matrix, which contains the of being in a particular state
and the state transition probability matrix, which contains the probability of changing from state $i$ to state $j$.

The Fontan's channel model is a three-state Markov chain model containing the following states:
\begin{itemize}
  \item \ac{los} (no shadowing)
  \item shadow (moderate shadowing conditions)
  \item deep shadow (heavy shadowing conditions)
\end{itemize}

In each state, the channel is modeled using Loo distribution~\cite{}. 
The overall received signal is described as a sum of the log-normally distributed direct component and Rayleigh distributed multipath component with parameters $\alpha$, $\psi$ and \MP.
The log-normal distribution is characterized by mean $\alpha$ and standard deviation $\psi$ and Rayleigh distribution by its average power \MP.
Different values for $\alpha$, $\psi$ and \MP are chosen depending on the state and elevation angle.
The \ac{pdf} is as follows:
\begin{equation}
  \begin{aligned}
  p(r) &= \frac{r}{b_0\sqrt{2\pi d_0}} \\[1ex]
  &\hphantom{=} \cdot \int_{0}^{\infty}\frac{1}{z} \exp\left[-\frac{(\ln z - \mu)^2}{2d_0} - \frac{r^2 + z^2}{2b_0}\right] \\[1ex]
  &\hphantom{=\int_{0}^{\infty}} \cdot I_0\Big(\frac{rz}{b_0}\Big)\ dz
  \end{aligned}
\end{equation}

whereby there the following dependencies exist between $mu$, $d_0$ and $b_0$ and $\alpha$, $\psi$ and \MP:
\begin{align}
\alpha &= 20\log_{10}(e^\mu) \\
  \psi &= 20\log_{10}(e^{\sqrt{d_0}}) \\
   \MP &= 10\log_{10}(2b_0)
\end{align}

Next, we consider the Gaussian noise $n$. We follow the approach presented in~\cite{LCN23}. 
We first compute the typical \ac{snr} values. 
To this end, we determine the distance $d$ between the satellite and the ground station based on the current elevation angle $\epsilon_0$ and Earth radius $R_E = 6378\,\textrm{km}$ as described in \cite{}:
%
\begin{equation}
  d = R_E\Biggl(\sqrt{\Bigl(\frac{h + R_E}{R_E}\Bigr)^2 - \cos^{2}\epsilon_{0}} - \sin\epsilon_{0}\Biggr),
\end{equation}
%

Next, we compute thermal noise $N$ as follows:
%
\begin{equation}
  N=k \cdot T \cdot B,
\end{equation}
%
where $B$ and $T$ denote the bandwidth and noise temperature respectively and $k = 1.380649 \cdot 10^{-23}$ is the Boltzmann's constant.
We compute the noise temperature as the sum of the antenna temperature $T_a = 290 K$ and the receiver noise temperature $T_e$, which is determined as:
%
\begin{equation}
  T_e = T_0 \big(F_{\mathrm{sys}} - 1 \big),
\end{equation}
%
$T_0 = 290$\,K is the reference temperature and $F_{\textrm{sys}} = 2$\,dB denotes the receiver's noise figure.

Next, we compute the path loss $L$ using the Friis formula \cite{} and determine the SNR depending on $L$ and $N$ as follows:
%
\begin{equation}
  \mathrm{SNR} = P_t + G_t + G_r - L - N,
\end{equation}
%
where $P_t$, $G_t$ and $G_r$ are input parameters denoting the transmitted power, the transmitter and the receiver gains respectively. 
The computed $\mathrm{SNR}$ value is then used to compute the noise power $\sigma^2$:
%
\begin{equation}
  \sigma^2 = \frac{P_{\mathrm{sig}}}{2 \cdot 10^{\frac{\mathrm{SNR}}{10}}}
\end{equation}
%
where $P_{\mathrm{sig}}$ is the normalized signal power.
Finally, we compute the noise vector $n$ as follows:
%
\begin{equation}
  n = \sigma \times \bigl[\mathcal{N}(0,1) + j * \mathcal{N}(0,1) \bigr] 
\end{equation}
%

%TODO:
% - z ersetzen weil doppelt
% - r, b0, d0, z erklären
% - Formel für h

\section{Evaluation}
\label{sec:evaluation}

\begin{table}
  \caption{Channel Parameters}
  \label{tab:channel_parameters}

  \centering
	\begin{tabular}{ll}
		\toprule
    Parameter & Value \\
    \midrule
		Orbit height & 150\,km \\
		Carrier frequency & 2150\,MHz \\
    Transmitted power & 1\,W \\
    Satellite antenna gain & 6\,dBi \\
    Ground station antenna gain & 35\,dBi \\
		Receive channel bandwidth & 750\,kHz \\
		Noise figure & 2\,dB \\
		\bottomrule
	\end{tabular}
\end{table}

In this section, we evaluate our mechanism (hereafter: \ours) and compare the results against a mechanism that also uses \ac{jscc} but employs a network architecture without attention modules, using a separate network for each channel condition (hereafter: \baseline).

Both mechanisms are trained using \sentinelii multi-spectral images (cf. \Cref{sec:system_model}) from the region of Serbia in summer, which were extracted from the BigEarth dataset~\cite{sumbul2019bigearthnet,Sumbul2021}.
After filtering out cloudy images, which do not contribute a valuable signal, 14,439 images remain, which we further split into training, validation, and test sets.
We use cubic interpolation to extrapolate all spectral bands to the same image resolution and represent pixel values as normalized values between 0 and 1.

During training, we set the batch size to 32; the learning rate is $10^{-3}$ and adjusted to $10^{-4}$ after 500 epochs; and Adam is used as a stochastic gradient descent.
All code was written using Keras\footnote{Website: https://keras.io/} and Tensorflow.\footnote{Website: https://www.tensorflow.org/}

To represent different channel conditions, we use statistical measurements for $\alpha$, $\psi$, and \MP obtained in different environments and channel states \cite{channel_values}.
These measurements were conducted for a range of elevation angles from 40\textdegree to 80\textdegree.
We further calculate the expected \ac{snr} as described in \Cref{sub:channel_model}.
As environments, we consider \emph{open, suburban, intermediate tree shadow, heavy tree shadow,} and \emph{urban.}
The channel states are differentiated with respect to their shadowing conditions as \emph{\acf{los}, shadow,} and \emph{deep shadow.}

We consider three evaluation scenarios:
%
\begin{enumerate}
  \item We evaluate how the \baseline performs in different environments, channel states, compression ratios, and elevation angles.
  \item We compare \ours against the \baseline to evaluate the impact of attention modules on the performance.
  \item We compare \ours against the \baseline in a scenario where the detected channel state differs from the actual channel state, and thus, the wrong network or attention module parametrization are used.
\end{enumerate}

For all scenarios, we compare the achieved image quality measured using \ac{psnr} -- higher values indicate better image quality.
Note that \ours uses an attention-module-parametrized version of the same network for a wide range of channel conditions.
We therefore expect \ours to perform slightly worse in terms of \ac{psnr} when compared to a custom tailored model.
The main advantage of \ours is that -- due to the parametrization -- the single model is much more storage efficient than individual model.
Moreover, the parameters for the attention modules can cater to nuanced differences in channel conditions, whereas individually trained models would likely cover a range of similar channel conditions to avoid the combinatorial explosion of storage requirements.
The main goal of our evaluation is, therefore, to determine whether the loss in \ac{psnr} is negligible when compared to the gain in storage efficiency.


\begin{figure*}[t!]
  \begin{subfigure}{.32\linewidth}
  \centering
  \input{plots/plt_no_af_40_los.tex}
  \caption{LOS}
  \label{subfig:baseline_los}
\end{subfigure}
\hfill
\begin{subfigure}{.32\linewidth}
  \centering
  \input{plots/plt_no_af_40_shadow.tex}
  \caption{Shadow}
  \label{subfig:baseline_shadow}
\end{subfigure}
\hfill
\begin{subfigure}{.32\linewidth}
  \centering
  \input{plots/plt_no_af_40_deep_shadow.tex}
  \caption{Deep shadow}
  \label{subfig:baseline_deep_shadow}
\end{subfigure}

\vspace{1em}
\centering
\ref{legendenv}

\caption{\ac{psnr} achieved by the \baseline for different environments, states, and compression ratios with 40\textdegree{} elevation angle.}
\label{fig:different_scenes_40}
\end{figure*}

First, we discuss the \baseline's results for different environments, states, and compression ratios with 40\textdegree{} elevation angle shown in \Cref{fig:different_scenes_40}. 
The $x$-axis shows different compression rates and the $y$-axis the achieved \ac{psnr} values.
It can be seen that the results strongly depend on the environment.
As expected, the best \ac{psnr} values are achieved in an open environment and the worst in more challenging environments, such as urban or intermediate tree shadow.
It also can be seen that the results significantly depend on shadowing conditions. 
While for deep shadow (\Cref{subfig:baseline_deep_shadow}), the performance varies considerably between different environments, 
the differences become less pronounced as shadowing conditions improve (\Cref{subfig:baseline_shadow,subfig:baseline_los}).

\begin{figure*}[t!]
  \begin{subfigure}{.32\linewidth}
  \centering
  \input{plots/plt_el_angle_0.04_los.tex}
  \caption{LOS, compression ratio 0.04}
  \label{subfig:base4080_los_04}
\end{subfigure}
\hfill
\begin{subfigure}{.32\linewidth}
  \centering
  \input{plots/plt_el_angle_0.04_shadow.tex}
  \caption{Shadow, compression ratio 0.04}
  \label{subfig:base4080_shadow_04}
\end{subfigure}
\hfill
\begin{subfigure}{.32\linewidth}
  \centering
  \input{plots/plt_el_angle_0.04_deep_shadow.tex}
  \caption{Deep shadow, compression ratio 0.04}
  \label{subfig:base4080_deep_shadow_04}
\end{subfigure}

\vspace{1em}

\begin{subfigure}{.32\linewidth}
  \centering
  \input{plots/plt_el_angle_0.33_los.tex}
  \caption{LOS, compression ratio 0.33}
  \label{subfig:base4080_los_33}
\end{subfigure}
\hfill
\begin{subfigure}{.32\linewidth}
  \centering
  \input{plots/plt_el_angle_0.33_shadow.tex}
  \caption{Shadow, compression ratio 0.33}
  \label{subfig:base4080_shadow_33}
\end{subfigure}
\hfill
\begin{subfigure}{.32\linewidth}
  \centering
  \input{plots/plt_el_angle_0.33_deep_shadow.tex}
  \caption{Deep shadow, compression ratio 0.33}
  \label{subfig:base4080_deep_shadow_33}
\end{subfigure}

\vspace{1em}
\centering
\ref{legendangles}
\tikz{%
  \node[draw=black, inner sep=3.55pt] {
    \tikz{\node[fill=plt1] {};} open \hspace{1em}
    \tikz{\node[fill=plt2] {};} suburban \hspace{1em}
    \tikz{\node[fill=plt3] {};} intermediate tree shadow \hspace{1em}
    \tikz{\node[fill=plt4] {};} heavy tree shadow \hspace{1em}
    \tikz{\node[fill=plt5] {};} urban
  };
}

\caption{\ac{psnr} achieved by the \baseline for different environments and 40\textdegree{} vs. 80\textdegree{} elevation angle.}
\label{fig:elevation_angles}
\end{figure*}

Next, we consider the differences between 40\textdegree{} and 80\textdegree{} elevation angles for the \baseline. 
The results for compression ratios of $k/n = 0.04$ and $k/n = 0.33$ are presented in \Cref{fig:elevation_angles}.
Again, the results depend both on the environment and the shadowing state.
While in \ac{los} conditions (\Cref{subfig:base4080_los_04,subfig:base4080_los_33}), the differences between 40\textdegree{} and 80\textdegree{} elevation angle remain negligible across different environments, 
stronger performance variations can be seen for the more challenging states shadow (\Cref{subfig:base4080_shadow_04,subfig:base4080_shadow_33}) and deep shadow (\Cref{subfig:base4080_deep_shadow_04,subfig:base4080_deep_shadow_33}).
In addition, we observe that the chosen compression rate influences the results depending on the elevation angle.
More specifically, the results suggest that moderate compression rates should be chosen in case of low elevation angles.


\begin{figure*}[t!]
  \begin{subfigure}{.48\linewidth}
  \centering
  \input{plots/plt_af_vs_no_af_urban_40.tex}
  \caption{40\textdegree{} elevation angle}
\end{subfigure}
\hfill
\begin{subfigure}{.48\linewidth}
  \centering
  \input{plots/plt_af_vs_no_af_urban_80.tex}
  \caption{80\textdegree{} elevation angle}
\end{subfigure}

\vspace{1em}
\centering
\ref{legendurban}

\caption{\ac{psnr} of \ours vs. \baseline in an urban environment.}
\label{fig:af_vs_no_af}
\end{figure*}

Next, we evaluate how the use of attention modules influences the performance.
We compare the neural network architectures with attention modules (\ours) and without attention modules (\baseline).
For our evaluation, we choose an urban environment since it shows stronger variability for different states and elevation angles.
The results are presented in \Cref{fig:af_vs_no_af}.
We can see that the performance of \ours depends on the compression ratio and the channel state and that the elevation angle plays a less important role. 
When stronger compression is applied, the \ac{psnr} values are similar to that of the \baseline, which uses separate neural network models for different conditions. 
In case of \ac{los} with a 40\textdegree{} elevation angle, \ours exhibits even slightly better performance than the \baseline.
For less aggressive compression, \ours shows slightly inferior results.
The biggest performance gap is observed in deep shadow conditions and the smallest in shadow conditions. 
This gap becomes even smaller for shadow conditions when the elevation angle is set to 80\textdegree{}. 
However, there is no significant difference between 40\textdegree{} and 80\textdegree{} elevation for the other channel states.

\begin{figure*}[t!]
  \begin{subfigure}{.48\linewidth}
  \centering
  \input{plots/plt_los_instead_of_deep_shadow_urban.tex}
  \caption{LOS instead of deep shadow (better than expected)}
  \label{fig:different_state_same_snr_better}
\end{subfigure}
\hfill
\begin{subfigure}{.48\linewidth}
  \centering
  \input{plots/plt_deep_shadow_instead_of_los_urban.tex}
  \caption{Deep shadow instead of LOS (worse than expected)}
  \label{fig:different_state_same_snr_worse}
\end{subfigure}

\vspace{1em}
\centering
\ref{legenderror}

\caption{Channel conditions differing from the expected state in an urban environment.}
\label{fig:different_state_same_snr}
\end{figure*}

Finally, we evaluate what happens when there is a mismatch between the estimated channel parameters and the real channel conditions. 
The underlying assumption is that both the satellite and the ground station constantly monitor the observed channel conditions and either select the most suitable neural network model (in case of the \baseline) or the most suitable attention module parameters (in case of \ours).
Therefore, it is possible that the wrong model is selected, which may lead to reduced performance.
The results are presented in \Cref{fig:different_state_same_snr}.
In \Cref{fig:different_state_same_snr_better}, assume the satellite and ground station both assume a deep shadow state but the actual state is \ac{los}. 
That is, the channel the conditions are better than expected. 
We compare the results for the correct channel estimation (solid lines) and wrong channel estimations (dashed lines).
The results show that both \ours and the \baseline are not able to benefit from the better channel state and experience noticeable performance degradation.
The results differ a lot when comparing 40\textdegree{} and 80\textdegree{} elevation angles.
While for 80\textdegree{}, both architectures achieve similar results, 
\ours performs significantly better for 40\textdegree{} elevation angle.
In \Cref{fig:different_state_same_snr_worse}, we consider the opposite case: 
A \ac{los} state is expected and the actual channel state is deep shadow. 
Similar to the previous example, the performance degrades in the case of the wrong state estimation.
For both elevation angles, \ours outperforms the \baseline.
Therefore, we observe that the use of attention module can bring additional benefits besides storage efficiency in cases where wrong channel conditions are assumed.

\section{Conclusion}
\label{sec:conclusion}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
